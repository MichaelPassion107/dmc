{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " recent vote to leave the european union and the rise of populist parties around the world.\n",
      "\n",
      "much of -->  \n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 78s - loss: 3.2186 - val_loss: 2.9849\n",
      "----- generating with seed: larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw the\n",
      "----- diversity: 0.5\n",
      "larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw theeoneo    eeraobes  oola  teoto e o esee,teoee s  it  eoe do ntte  tnomeiy eeyrcae i h  e rl en3ea h \n",
      "----- diversity: 1.2\n",
      "larger and more front-loaded fiscal stimulus than even president roosevelts new deal and oversaw theeune nac s\n",
      "fsnft  umgnuouinad?der:bswahn l\n",
      "alwdzvo llag lptonn   qaglawt; ufq6rhfhviceqeeseiwl0ysomg\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 3.0123 - val_loss: 2.9409\n",
      "----- generating with seed: an for those at the top see chart 2. under my administration, we will have boosted incomes for famil\n",
      "----- diversity: 0.5\n",
      "an for those at the top see chart 2. under my administration, we will have boosted incomes for famil. u nrenlos  dbet r wer d otig eav  nn saoi we sotpiet  gin a rant eltootioreat  rl ternhoses   tosr\n",
      "----- diversity: 1.2\n",
      "an for those at the top see chart 2. under my administration, we will have boosted incomes for famil- .yaecymfif l,s,e,fwlce  awrfor rdsgfdeofr.e,9.oeviahceaepoenrs4elried2heoiepbiv ovytrcsbous,enlnmm\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 88s - loss: 2.9436 - val_loss: 2.8762\n",
      "----- generating with seed: for the most fortunate can address long-term fiscal challenges without sacrificing investments in gr\n",
      "----- diversity: 0.5\n",
      "for the most fortunate can address long-term fiscal challenges without sacrificing investments in grogas  rh onsl   toroeo s nadis het lo a orod o acsit ne cacder  aes tt h eaoe meti it  asi ae et nnl\n",
      "----- diversity: 1.2\n",
      "for the most fortunate can address long-term fiscal challenges without sacrificing investments in gr hmhua0n enbfsd eaos6ein  gyro not  ofdekvb airie.doan rwhi,uptn tee i rmtmad bo pou o utacodt rcaco\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.8399 - val_loss: 2.7510\n",
      "----- generating with seed: ises a return to a past that is not possible to restoreand that, for most americans, never existed a\n",
      "----- diversity: 0.5\n",
      "ises a return to a past that is not possible to restoreand that, for most americans, never existed aoo tto haav thenes at a reh  noon  otin te rie res an to tear or tan ti ds an aai n th aih at ont ia\n",
      "----- diversity: 1.2\n",
      "ises a return to a past that is not possible to restoreand that, for most americans, never existed an tuobmst tow deuaw senmedennymegene g,e cmsanlve ktaw csltl. iss oswyr nalr7fsthprvrteahs yhargando\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 2.7221 - val_loss: 2.6342\n",
      "----- generating with seed: remember that capitalism has been the greatest driver of prosperity and opportunity the world has ev\n",
      "----- diversity: 0.5\n",
      "remember that capitalism has been the greatest driver of prosperity and opportunity the world has ever tirtt ep odet se re te oor ans the te ahe ar ani are te sed tus eoo the t he thereee ,e the ae pe\n",
      "----- diversity: 1.2\n",
      "remember that capitalism has been the greatest driver of prosperity and opportunity the world has evegegcmle  eaeonli. htsginnorgiotag pireoso celt b2awe 0maslth nroie polns  oouainvk4r6 hcvup inoctm \n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 76s - loss: 2.6210 - val_loss: 2.5534\n",
      "----- generating with seed: al physicists and engineers spend their careers shifting money around in the financial sector, inste\n",
      "----- diversity: 0.5\n",
      "al physicists and engineers spend their careers shifting money around in the financial sector, inste thos aa eo ale aot anmes the anr owre anw eotis ace orin oar coith t ace rot ant eor the shaoe mon \n",
      "----- diversity: 1.2\n",
      "al physicists and engineers spend their careers shifting money around in the financial sector, insteoogem  haotorod gep caetminr.ppivhitt iccc?ot or-u7islgl.eset fmoort the byhono aesuri\n",
      "toenb akaerig\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 72s - loss: 2.5585 - val_loss: 2.4987\n",
      "----- generating with seed: and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector jo\n",
      "----- diversity: 0.5\n",
      "and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector jon re ale san and thes mhoat ins poon are is the the ane fort the be indoane the shinte aos ang eror \n",
      "----- diversity: 1.2\n",
      "and power plants.\n",
      "\n",
      "the results are clear: a more durable, growing economy; 15m new private-sector joontde g oelrkit, ons fioloug tomlycorthufg ro l3euoiyy, 5ovs anleofpcvottha ih bathylitaiadita\n",
      "telt \n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 2.5001 - val_loss: 2.4498\n",
      "----- generating with seed: ithout dependent children, limiting tax breaks for high-income households, preventing colleges from \n",
      "----- diversity: 0.5\n",
      "ithout dependent children, limiting tax breaks for high-income households, preventing colleges from aper an ond wes red the care the romere sores ans or the peblinc on the the ant to el mon ent coon p\n",
      "----- diversity: 1.2\n",
      "ithout dependent children, limiting tax breaks for high-income households, preventing colleges from tujod enbeistewpvesd terestq.ame,e amalolt l, ces,teqluien ua dvechesofs anti.ad yp\n",
      "onri\n",
      "avin caprle\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.4564 - val_loss: 2.4114\n",
      "----- generating with seed: cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnership\n",
      "----- diversity: 0.5\n",
      "cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnership de and isep pond the d douth as the presst th ar redare er erost an ane raand ab to rod in atig ad \n",
      "----- diversity: 1.2\n",
      "cil of economic advisers. so, i will keep pushing for congress to pass the trans-pacific partnership tniricdinnh aurete tce hes modiunhet and rtca.ls,p bn le0e ?perceonmgsd ws ikelbat-teangeas, tge ta\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 77s - loss: 2.4175 - val_loss: 2.3803\n",
      "----- generating with seed: ding a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more r\n",
      "----- diversity: 0.5\n",
      "ding a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more rore an indith il int o wad reat on ins on the por ans ant pores and roes the and an bualit an and pa\n",
      "----- diversity: 1.2\n",
      "ding a sturdier foundation\n",
      "finally, the financial crisis painfully underscored the need for a more ras eex puatit tave ioucoffxn0tinhwert 3oplomd, vhits coider, ivertrthe facincm prm-de mredimuss onth\n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 2.3819 - val_loss: 2.3523\n",
      "----- generating with seed:  its not new, nor is it dissimilar to a discontent spreading throughout the world, often manifested \n",
      "----- diversity: 0.5\n",
      " its not new, nor is it dissimilar to a discontent spreading throughout the world, often manifested bos ease an en butis ansing comermerith wo the the sepand the sing the the ans in pones an oor cias \n",
      "----- diversity: 1.2\n",
      " its not new, nor is it dissimilar to a discontent spreading throughout the world, often manifested mhasthfor l;odrorsty :xdt-lurvirithr mreat theg uorlmh iponcidode the ,or iinm y ialutgotiicrincitis\n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 72s - loss: 2.3541 - val_loss: 2.3295\n",
      "----- generating with seed: nd opportunity the world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in\n",
      "----- diversity: 0.5\n",
      "nd opportunity the world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living in some forate an in pore wer the pon tare fan ar lunthe santee enon poucering the the conres on pore \n",
      "----- diversity: 1.2\n",
      "nd opportunity the world has ever known.\n",
      "\n",
      "over the past 25 years, the proportion of people living int agnxwme anbe eentte pnonvicl popu; tors cerigbiconag,es e forl mtee whle uol ttebte p orelrtse non\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 2.3281 - val_loss: 2.3094\n",
      "----- generating with seed: n short-term funding, and better oversight for a range of institutions and markets. big american fin\n",
      "----- diversity: 0.5\n",
      "n short-term funding, and better oversight for a range of institutions and markets. big american find ard ther the wist an ion the toren bat the thas an the se ald than theo thal are ind the on the co\n",
      "----- diversity: 1.2\n",
      "n short-term funding, and better oversight for a range of institutions and markets. big american finesutid tce inob the pwbvive., uhos bandysisonting.\n",
      "nae perlucer sy-ciceiral deldite sessent the. orl\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 2.2990 - val_loss: 2.2938\n",
      "----- generating with seed: n 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had m\n",
      "----- diversity: 0.5\n",
      "n 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had mires ol joor the poredes an our tore the than antore an iof rleand the thour ing norke the  ar mere \n",
      "----- diversity: 1.2\n",
      "n 1979, the top 1 of american families received 7 of all after-tax income. by 2007, that share had miintanagrissimit of conoling covet4edpinausthaa foveesy 5ciint iilgijeibat fo1ebiyn shuty icy,\n",
      "ijnc-\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 2.2785 - val_loss: 2.2728\n",
      "----- generating with seed:  todays low interest rates, fiscal policy must play a bigger role in combating future downturns; mon\n",
      "----- diversity: 0.5\n",
      " todays low interest rates, fiscal policy must play a bigger role in combating future downturns; mon as for the selanco thin the the thar the tho the the in the the thas or ant the tines al in one the\n",
      "----- diversity: 1.2\n",
      " todays low interest rates, fiscal policy must play a bigger role in combating future downturns; mon. wed tysunp toulret guobley sha nh i0gux geunh sou.usk, esderon tot ad tfrrety bie9 g-leulving scar\n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 72s - loss: 2.2537 - val_loss: 2.2537\n",
      "----- generating with seed: er to lose your place at the top.\n",
      "\n",
      "economists have listed many causes for the rise of inequality: te\n",
      "----- diversity: 0.5\n",
      "er to lose your place at the top.\n",
      "\n",
      "economists have listed many causes for the rise of inequality: ter the fore in bitingert the pores and the an the the the the mer and has acing that of en for potith\n",
      "----- diversity: 1.2\n",
      "er to lose your place at the top.\n",
      "\n",
      "economists have listed many causes for the rise of inequality: te hhantarsd thwsads tuamtid rued d bulenond buryecfin cook ,. maeaibnitihaw im rucwititaatiin mouhe c\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 2.2295 - val_loss: 2.2406\n",
      "----- generating with seed:  shocks before they occur. with todays low interest rates, fiscal policy must play a bigger role in \n",
      "----- diversity: 0.5\n",
      " shocks before they occur. with todays low interest rates, fiscal policy must play a bigger role in thes tinge the dengrose be werke pritice the weres on recenting the sate in conting the won the that\n",
      "----- diversity: 1.2\n",
      " shocks before they occur. with todays low interest rates, fiscal policy must play a bigger role in re intrcy costicpisinusi-voriving,, ster ifaryy. ucskrectond s:smamed ledulemd tou ecrgeoming antuia\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 2.2179 - val_loss: 2.2309\n",
      "----- generating with seed: p with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for \n",
      "----- diversity: 0.5\n",
      "p with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for the to tian and ancer an corens tre in anl were in souste but in the dens batian prone the to stor t\n",
      "----- diversity: 1.2\n",
      "p with the eu. these agreements, and stepped-up trade enforcement, will level the playing field for the umity ol athomt vititad ford fherl ande poo the. jait checming ingu thet rsoremwto, lohe bty pyg\n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.1938 - val_loss: 2.2189\n",
      "----- generating with seed: s a job. however, america has faced a long-term decline in participation among prime-age workers see\n",
      "----- diversity: 0.5\n",
      "s a job. however, america has faced a long-term decline in participation among prime-age workers see and at the and mechem the to dersease for the pronders growth the to cell and at alre and tos and r\n",
      "----- diversity: 1.2\n",
      "s a job. however, america has faced a long-term decline in participation among prime-age workers seen ald conline whe ivtlisacorinisr oraufe coviill- edyed at. ennouse the exenst tartnc aedtsroncancy \n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 69s - loss: 2.1716 - val_loss: 2.2043\n",
      "----- generating with seed:  should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the e\n",
      "----- diversity: 0.5\n",
      " should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the ent the s contre the ant and the tane and sumite ferthe ses the aven and pante in on and to part ant \n",
      "----- diversity: 1.2\n",
      " should rise automatically.\n",
      "\n",
      "maintaining fiscal discipline in good times to expand support for the ed mane te hal .9pcaninss. \n",
      "fvin8-dct0nt last tha ot brsigncerdey anu uuminitsy sh, hresth-inges fis \n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 75s - loss: 2.1510 - val_loss: 2.1940\n",
      "----- generating with seed: ing the future at the service of the present. there should no longer be any doubt that a free market\n",
      "----- diversity: 0.5\n",
      "ing the future at the service of the present. there should no longer be any doubt that a free market wof ensoris al as al mene some the pore the the the the profi-tore andersesthe tomer ald ince andin\n",
      "----- diversity: 1.2\n",
      "ing the future at the service of the present. there should no longer be any doubt that a free market mhoud americand s apoit2oges ow h maece mapt matod duleoly hmofcen theduant ath, and\n",
      "alse thabby be\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 91s - loss: 2.1288 - val_loss: 2.1852\n",
      "----- generating with seed: many potential physicists and engineers spend their careers shifting money around in the financial s\n",
      "----- diversity: 0.5\n",
      "many potential physicists and engineers spend their careers shifting money around in the financial sacing that in and it are arle soul or at and geand thit in incanimy cond bis porting res for the ses\n",
      "----- diversity: 1.2\n",
      "many potential physicists and engineers spend their careers shifting money around in the financial sgrincn anng9he ctrvitberuplocte, foice. iher excsrowor gee wert moco, s ater. abdi;etming\n",
      "e cost\n",
      "ona\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 95s - loss: 2.1085 - val_loss: 2.1705\n",
      "----- generating with seed: ries with greater inequality. concentrated wealth at the top means less of the broad-based consumer \n",
      "----- diversity: 0.5\n",
      "ries with greater inequality. concentrated wealth at the top means less of the broad-based consumer somolise and on that or the or and work restert as ald on inentoning songes male the ane noshers in \n",
      "----- diversity: 1.2\n",
      "ries with greater inequality. concentrated wealth at the top means less of the broad-based consumer atqurecint contomy ros und1reclygat iak  alre. mo\n",
      "uscibies wuv wiali piim notsherpateialira. the afs\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 92s - loss: 2.0980 - val_loss: 2.1642\n",
      "----- generating with seed:  eras in which americans were told they could restore past glory if they just got some group or idea\n",
      "----- diversity: 0.5\n",
      " eras in which americans were told they could restore past glory if they just got some group or ideant and rade sy acd thal and bosyding the the non and the put roid in us to prowt cand prolle cas to \n",
      "----- diversity: 1.2\n",
      " eras in which americans were told they could restore past glory if they just got some group or idea. ot banigtr thiel ditiugiand. wor ofy roultirad adpe paly y jonkr\n",
      "w hoft 8l.s4erncamid aycinl of te\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 2.0752 - val_loss: 2.1541\n",
      "----- generating with seed: debate how best to build on these rules, but denying that progress leaves us more vulnerable, not le\n",
      "----- diversity: 0.5\n",
      "debate how best to build on these rules, but denying that progress leaves us more vulnerable, not less sore wor chal contiming the seasiting the ancen tomking the pross avere of prine and for but or t\n",
      "----- diversity: 1.2\n",
      "debate how best to build on these rules, but denying that progress leaves us more vulnerable, not lett yeosde lme esow.le savei for nestunabe. lan beraclisat coverkense. to-merkss l0m mivink thle t-on\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 96s - loss: 2.0531 - val_loss: 2.1489\n",
      "----- generating with seed: ontinue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress an\n",
      "----- diversity: 0.5\n",
      "ontinue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress an what hew enderen and al prover ad in reaning the conerss for acenome and tur as the the ported sers\n",
      "----- diversity: 1.2\n",
      "ontinue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this paradox of progress and chorhetr odt. qmorssfablf 2ite ias 0io. eadmititiogyh;nowbe noubalramshat atconcave enorasss ghe p\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 76s - loss: 2.0462 - val_loss: 2.1428\n",
      "----- generating with seed: f all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very es\n",
      "----- diversity: 0.5\n",
      "f all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very es the belunt the ofrens the desrel chapres in the to and in alle pare and in the tipall sandes the th\n",
      "----- diversity: 1.2\n",
      "f all after-tax income. by 2007, that share had more than doubled to 17. this challenges the very eso\n",
      " wele tsl lesomhec dothehs thoiedhal, encequal tle topy dtraciinecslsted munte forioum inthett, in\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 69s - loss: 2.0204 - val_loss: 2.1364\n",
      "----- generating with seed: , while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly three-\n",
      "----- diversity: 0.5\n",
      ", while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly three-reasing of the the the desint and resinale an ias more se poltitising the pastiog of the paod the th\n",
      "----- diversity: 1.2\n",
      ", while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nearly three- wafdaves foj,es.: sisens, thes bocon0 noucthaig iss eemitipes hiesed yevormecs ade comled-ofte. bt \n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 2.0073 - val_loss: 2.1297\n",
      "----- generating with seed: for each commonsense measure expended substantial energy. i did not get some of the expansions i sou\n",
      "----- diversity: 0.5\n",
      "for each commonsense measure expended substantial energy. i did not get some of the expansions i soulity an the fare for that shall dever condenc and lisen in ine and in ast a thing ald and in the wer\n",
      "----- diversity: 1.2\n",
      "for each commonsense measure expended substantial energy. i did not get some of the expansions i souucil the cfostoy hal qeond consmin ans tow. huk if and bhes to bet eraa re 3e got arce 28t ce:ttom j\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 63s - loss: 1.9865 - val_loss: 2.1212\n",
      "----- generating with seed:  was stabilised without costing taxpayers a dime and the auto industry rescued. i enacted a larger a\n",
      "----- diversity: 0.5\n",
      " was stabilised without costing taxpayers a dime and the auto industry rescued. i enacted a larger and buting to the for dealk rese the and in the to the thand burice so porces and ande ande the ingra\n",
      "----- diversity: 1.2\n",
      " was stabilised without costing taxpayers a dime and the auto industry rescued. i enacted a larger aperination on ceecuntica, d sgvented oin radacurd gro ther ofgrotican incilombticaty whot wonc jobl \n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 74s - loss: 1.9725 - val_loss: 2.1244\n",
      "----- generating with seed: forces. decades of declining productivity growth and rising inequality have resulted in slower incom\n",
      "----- diversity: 0.5\n",
      "forces. decades of declining productivity growth and rising inequality have resulted in slower income the the rowes in whe and encretiding the anderst in the enor contere and endenture and dicing ta t\n",
      "----- diversity: 1.2\n",
      "forces. decades of declining productivity growth and rising inequality have resulted in slower income mudeens eo arssweg has se7upe sing sowleclf vetirsnat ot com bitstinntees herkedive daosd boofr-se\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 84s - loss: 1.9513 - val_loss: 2.1187\n",
      "----- generating with seed: ions and markets. big american financial institutions no longer get the type of easier funding they \n",
      "----- diversity: 0.5\n",
      "ions and markets. big american financial institutions no longer get the type of easier funding they lore un the for cand tion shat at rost restare tha thal the and ser an contime shat ever geow the an\n",
      "----- diversity: 1.2\n",
      "ions and markets. big american financial institutions no longer get the type of easier funding they for phopleleon thenumpricterjasent cusquesun wey vithus. nits thit sy aud shalimoxesrot syefisusth a\n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 88s - loss: 1.9385 - val_loss: 2.1294\n",
      "----- generating with seed: number of eras in which americans were told they could restore past glory if they just got some grou\n",
      "----- diversity: 0.5\n",
      "number of eras in which americans were told they could restore past glory if they just got some grousher anderty and the welden ame ant rabitishand the inconges more antimess to ger the proares of the\n",
      "----- diversity: 1.2\n",
      "number of eras in which americans were told they could restore past glory if they just got some groume, thes the mprilas angenet batunetrte tig radershsforgceow blennczttodr ed tho weusdprkisn more al\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.9235 - val_loss: 2.1157\n",
      "----- generating with seed: dren, particularly for infrastructure; and a political system so partisan that previously bipartisan\n",
      "----- diversity: 0.5\n",
      "dren, particularly for infrastructure; and a political system so partisan that previously bipartisaning the toren thas the the forderss the andes for an ingerante an bote beonden vereniveres the ingra\n",
      "----- diversity: 1.2\n",
      "dren, particularly for infrastructure; and a political system so partisan that previously bipartisanby by compitcady; secfisiab io alncassed to sol papticave .\n",
      "obo cuvestyem addeequation toaldts ow re\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 65s - loss: 1.9124 - val_loss: 2.1115\n",
      "----- generating with seed: ious administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need\n",
      "----- diversity: 0.5\n",
      "ious administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need the pistisine fouthal conger for ento te han werken wed the tabue the lontiming to partititis roste\n",
      "----- diversity: 1.2\n",
      "ious administration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need lutying se at1 o5.  it fre s. over tostuino congomming tealpirat9y kev tathen whed rullly, receneri\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.8923 - val_loss: 2.1123\n",
      "----- generating with seed: ly childhood education to improving high schools, making college more affordable and expanding high-\n",
      "----- diversity: 0.5\n",
      "ly childhood education to improving high schools, making college more affordable and expanding high-antert and forcer ald contere soude and a dorecat or amerisang ges lion seas economy seme the wall w\n",
      "----- diversity: 1.2\n",
      "ly childhood education to improving high schools, making college more affordable and expanding high--utpreoneaens pnossemon. erewon ty ahl 1mendem deisdelingst oun thit. thaudm. oap inoval ag on acabe\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 64s - loss: 1.8746 - val_loss: 2.1166\n",
      "----- generating with seed: umented, the failure of businesses to take into account the impact of their decisions on others thro\n",
      "----- diversity: 0.5\n",
      "umented, the failure of businesses to take into account the impact of their decisions on others throw the that the besterican be gronter ig of and the and in mare ann the ther growth has  ande in the \n",
      "----- diversity: 1.2\n",
      "umented, the failure of businesses to take into account the impact of their decisions on others throges that falings ic hes allhane  eocutcex\n",
      "\n",
      "fox thes surugrenhstylitior fyrecconded myx5by botomens a\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.8541 - val_loss: 2.1146\n",
      "----- generating with seed: or a more resilient economy, one that grows sustainably without plundering the future at the service\n",
      "----- diversity: 0.5\n",
      "or a more resilient economy, one that grows sustainably without plundering the future at the service ous that and in alition in mere the to more whand and and mane the the sond and in ant oum the aner\n",
      "----- diversity: 1.2\n",
      "or a more resilient economy, one that grows sustainably without plundering the future at the service tact mas edetos e. thac thex im noaly for tam arest gmonapilathy sowh  en9plins hogretser 5heg andi\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 79s - loss: 1.8356 - val_loss: 2.1157\n",
      "----- generating with seed: e americans in the labour market when they fall on hard times. these include providing wage insuranc\n",
      "----- diversity: 0.5\n",
      "e americans in the labour market when they fall on hard times. these include providing wage insurance tha the partinas and tion wal that the pare and reave to ale inamitiobte in poorting roven bel ain\n",
      "----- diversity: 1.2\n",
      "e americans in the labour market when they fall on hard times. these include providing wage insurance with-callet michaasingst for encurtag insonged sion myinmumabl cay for mopequallente an to1 abrele\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 76s - loss: 1.8267 - val_loss: 2.1077\n",
      "----- generating with seed: comprehensive rewriting of the rules of the financial system since the 1930s, as well as reforming h\n",
      "----- diversity: 0.5\n",
      "comprehensive rewriting of the rules of the financial system since the 1930s, as well as reforming hable andes to bolieg in the the mare and rester. the proding ou  anle pall semers sade sure and to b\n",
      "----- diversity: 1.2\n",
      "comprehensive rewriting of the rules of the financial system since the 1930s, as well as reforming heamenty dits. to hora-tort, wir; eupore shersexr on. 1mevenst3 falilassikg whs comed, and eould besu\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 72s - loss: 1.8045 - val_loss: 2.1109\n",
      "----- generating with seed: in the real economy. and the financial crisis of 2008 only seemed to increase the isolation of corpo\n",
      "----- diversity: 0.5\n",
      "in the real economy. and the financial crisis of 2008 only seemed to increase the isolation of corporeis that des io tice and to preverige jof the prowed for can in ored to are ane rave for the werken\n",
      "----- diversity: 1.2\n",
      "in the real economy. and the financial crisis of 2008 only seemed to increase the isolation of corpores a dked st sseenjacing of a duritithan trgod henlo-polimus ancurcame.\n",
      "\n",
      "imsheof pet thj grows and \n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.7856 - val_loss: 2.1100\n",
      "----- generating with seed:  advanced economies see chart 1. without a faster-growing economy, we will not be able to generate t\n",
      "----- diversity: 0.5\n",
      " advanced economies see chart 1. without a faster-growing economy, we will not be able to generate the angerens in the illong the growurd ad the worid an tof the geostere sonited that be and nomerste \n",
      "----- diversity: 1.2\n",
      " advanced economies see chart 1. without a faster-growing economy, we will not be able to generate th op ver.\n",
      "\n",
      "ectorg taon exdisrensings. mood ano woust re9s tu\n",
      " verdeules fon baok requalmements facri\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.7787 - val_loss: 2.1100\n",
      "----- generating with seed: ng of the rules of the financial system since the 1930s, as well as reforming health care and introd\n",
      "----- diversity: 0.5\n",
      "ng of the rules of the financial system since the 1930s, as well as reforming health care and introditigress and and the in cand the sowithes dang the for eforme growth resoule the cfortica suplon res\n",
      "----- diversity: 1.2\n",
      "ng of the rules of the financial system since the 1930s, as well as reforming health care and introded is mitinn devorigs-stolg wtican mons buttar jay cans seage hall choioge avetable tiy beaod silles\n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 1.7520 - val_loss: 2.1160\n",
      "----- generating with seed: add flexibility for employees and employers. reforms to our criminal-justice system and improvements\n",
      "----- diversity: 0.5\n",
      "add flexibility for employees and employers. reforms to our criminal-justice system and improvements denaticals condem sowe cantire sostipand cassed the fare and camend and in omeminiss and incastirin\n",
      "----- diversity: 1.2\n",
      "add flexibility for employees and employers. reforms to our criminal-justice system and improvements beating aver los urd muklos ef wezend a camy howorke foh thlealt eantion fur epprodders oncene cosl\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.7410 - val_loss: 2.1105\n",
      "----- generating with seed: oadly shared prosperity and growth. economists have long recognised that markets, left to their own \n",
      "----- diversity: 0.5\n",
      "oadly shared prosperity and growth. economists have long recognised that markets, left to their own incedeund mone thes sorees to ereas to lepesand that and antion to tor s. the the pars dive in the t\n",
      "----- diversity: 1.2\n",
      "oadly shared prosperity and growth. economists have long recognised that markets, left to their own tti-iestima ecsicnos an acy anrerc\n",
      "coprtiind, infrtlsem adtroanc, spoovertof stoned  how bes to kros\n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.7215 - val_loss: 2.1181\n",
      "----- generating with seed: that progress is possible. last year, income gains were larger for households at the bottom and midd\n",
      "----- diversity: 0.5\n",
      "that progress is possible. last year, income gains were larger for households at the bottom and midd and tha derpating on were anders to bule and ener ener and in ane growth the and bate and the ale a\n",
      "----- diversity: 1.2\n",
      "that progress is possible. last year, income gains were larger for households at the bottom and midd oed afe wandure vey deaclicmertsegr aplexs amequand 6f ic fouls ald ptille-sotomy crasto 4e ayebun \n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.7109 - val_loss: 2.1187\n",
      "----- generating with seed: th care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are\n",
      "----- diversity: 0.5\n",
      "th care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are and enon com ad and in sod and so more for the paring the more 100s the soune whe pontition wa amer\n",
      "----- diversity: 1.2\n",
      "th care and introducing new rules cutting emissions from vehicles and power plants.\n",
      "\n",
      "the results are vile the 1906t ecwanevatee con, e umerste on to weh the than toed trew hich-pinaly os tore her xfin\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 68s - loss: 1.7047 - val_loss: 2.1246\n",
      "----- generating with seed:  in extreme poverty has fallen from nearly 40 to under 10. last year, american households enjoyed th\n",
      "----- diversity: 0.5\n",
      " in extreme poverty has fallen from nearly 40 to under 10. last year, american households enjoyed the andedsing of on sistore to porking the to boun for as in the and on the as geant at in and in mare\n",
      "----- diversity: 1.2\n",
      " in extreme poverty has fallen from nearly 40 to under 10. last year, american households enjoyed tha msothen groomrhhes andrees h surkns falonk-nncctimmcessarlo. ains rofines pealuts on stempticotay \n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 67s - loss: 1.6839 - val_loss: 2.1307\n",
      "----- generating with seed: dworking americans can get ahead requires addressing four major structural challenges: boosting prod\n",
      "----- diversity: 0.5\n",
      "dworking americans can get ahead requires addressing four major structural challenges: boosting productivity winc mart the the inormont ad poricts and devured chosten shate pant aed the wand prowth an\n",
      "----- diversity: 1.2\n",
      "dworking americans can get ahead requires addressing four major structural challenges: boosting prodling hes to-umeris cantihitser gof rcans ued stsy. eax hhiasive fuc ut5rese sol ech oprtrescr ghovun\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 66s - loss: 1.6602 - val_loss: 2.1365\n",
      "----- generating with seed: sm and markets cannot continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this\n",
      "----- diversity: 0.5\n",
      "sm and markets cannot continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this sould the will conger sters empreding of the in os and ande that be forces to eenpronter sulse won \n",
      "----- diversity: 1.2\n",
      "sm and markets cannot continue to deliver the gains they have delivered in the past centuries.\n",
      "\n",
      "this, 118 cerpee tobean t cad efom ewcrekudto cedtt nor grobstop hmanbtiincs enjulc syssec in maec houch\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
